{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRivSvBpOG8J"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/thanhnew2001/songcover\n",
        "%cd /content/songcover"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "FFHMiOPWOU6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define all the functions:\n",
        "# -*- coding: utf-8 -*-\n",
        "import os\n",
        "import subprocess\n",
        "import json\n",
        "import numpy as np\n",
        "from scipy.io import wavfile\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime, timedelta\n",
        "from pydub import AudioSegment\n",
        "\n",
        "# Separate audio using Demucs\n",
        "def separate_audio(input_audio):\n",
        "    command = f\"demucs --two-stems=vocals {input_audio}\"\n",
        "\n",
        "    print(\"Separating audio into vocals and instruments...\")\n",
        "    result = subprocess.run(command.split(), stdout=subprocess.PIPE)\n",
        "    print(result.stdout.decode())\n",
        "    print(\"Audio separation completed.\")\n",
        "\n",
        "def GetTime(video_seconds):\n",
        "    if video_seconds < 0:\n",
        "        return \"00:00:00.001\"\n",
        "    else:\n",
        "        sec = timedelta(seconds=float(video_seconds))\n",
        "        d = datetime(1, 1, 1) + sec\n",
        "        return f\"{str(d.hour).zfill(2)}:{str(d.minute).zfill(2)}:{str(d.second).zfill(2)}.001\"\n",
        "\n",
        "def windows(signal, window_size, step_size):\n",
        "    for i_start in range(0, len(signal), step_size):\n",
        "        i_end = i_start + window_size\n",
        "        if i_end >= len(signal):\n",
        "            break\n",
        "        yield signal[i_start:i_end]\n",
        "\n",
        "def energy(samples):\n",
        "    return np.sum(np.power(samples, 2.)) / float(len(samples))\n",
        "\n",
        "def rising_edges(binary_signal):\n",
        "    previous_value = 0\n",
        "    index = 0\n",
        "    for x in binary_signal:\n",
        "        if x and not previous_value:\n",
        "            yield index\n",
        "        previous_value = x\n",
        "        index += 1\n",
        "\n",
        "def split_audio(input_file, output_dir, min_silence_length=0.6, silence_threshold=1e-4, step_duration=0.003):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"Splitting audio file {input_file}...\")\n",
        "    sample_rate, samples = wavfile.read(input_file)\n",
        "    max_amplitude = np.iinfo(samples.dtype).max\n",
        "    max_energy = energy([max_amplitude])\n",
        "    window_size = int(min_silence_length * sample_rate)\n",
        "    step_size = int(step_duration * sample_rate)\n",
        "\n",
        "    signal_windows = windows(samples, window_size, step_size)\n",
        "    window_energy = (energy(w) / max_energy for w in tqdm(signal_windows))\n",
        "    window_silence = (e > silence_threshold for e in window_energy)\n",
        "    cut_times = (r * step_duration for r in rising_edges(window_silence))\n",
        "\n",
        "    cut_samples = [int(t * sample_rate) for t in cut_times]\n",
        "    cut_samples.append(-1)\n",
        "    cut_ranges = [(i, cut_samples[i], cut_samples[i + 1]) for i in range(len(cut_samples) - 1)]\n",
        "\n",
        "    video_sub = {str(i): [str(GetTime(cut_samples[i] / sample_rate)),\n",
        "                          str(GetTime(cut_samples[i + 1] / sample_rate))]\n",
        "                  for i in range(len(cut_samples) - 1)}\n",
        "\n",
        "    for i, start, stop in tqdm(cut_ranges):\n",
        "        output_file_path = os.path.join(output_dir, f\"{os.path.splitext(os.path.basename(input_file))[0]}_{i:03d}.wav\")\n",
        "        wavfile.write(output_file_path, rate=sample_rate, data=samples[start:stop])\n",
        "        print(f\"Written file: {output_file_path}\")\n",
        "\n",
        "    with open(os.path.join(output_dir, f\"{os.path.splitext(os.path.basename(input_file))[0]}.json\"), 'w') as output:\n",
        "        json.dump(video_sub, output)\n",
        "\n",
        "    print(\"Audio splitting completed.\")\n",
        "\n",
        "\n",
        "# Preprocessing step\n",
        "def preprocess():\n",
        "    print(\"Preprocessing for training...\")\n",
        "    subprocess.run(['svc', 'pre-resample'])\n",
        "    subprocess.run(['svc', 'pre-config'])\n",
        "\n",
        "    subprocess.run(['svc', 'pre-hubert', '-fm', 'dio'])\n",
        "\n",
        "    print(\"Preprocessing completed.\")\n",
        "\n",
        "# Training step\n",
        "def train_model():\n",
        "    print(\"Training model...\")\n",
        "    subprocess.run(['svc', 'train', '--model-path', 'logs/44k'])\n",
        "    print(\"Training completed.\")\n",
        "\n",
        "# Combine vocal and instrument\n",
        "def combine_audio(vocal_file, instrument_file, output_file):\n",
        "    print(\"Combining vocal and instrument...\")\n",
        "    sound1 = AudioSegment.from_file(vocal_file)\n",
        "    sound2 = AudioSegment.from_file(instrument_file)\n",
        "    combined = sound1.overlay(sound2)\n",
        "    combined.export(output_file, format='wav')\n",
        "    print(f\"Combined audio saved to {output_file}\")\n",
        "\n",
        "speaker_name = \"thanh\"\n",
        "audio_input = \"thanh.wav\"  # Replace with your MP3 path\n",
        "separate_audio(audio_input)\n",
        "\n",
        "output_dir = f\"dataset_raw/{speaker_name}\"\n",
        "split_audio(f\"separated/htdemucs/{speaker_name}/vocals.wav\", output_dir)\n",
        "\n",
        "preprocess()  # Preprocess the data\n",
        "train_model()  # Train the model\n"
      ],
      "metadata": {
        "id": "aequLPzrQNbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download a song you want to cover"
      ],
      "metadata": {
        "id": "tvPI-iutVJXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download Youtube WAV\n",
        "from __future__ import unicode_literals\n",
        "import yt_dlp\n",
        "import ffmpeg\n",
        "import sys\n",
        "\n",
        "ydl_opts = {\n",
        "    'format': 'bestaudio/best',\n",
        "#    'outtmpl': 'output.%(ext)s',\n",
        "    'postprocessors': [{\n",
        "        'key': 'FFmpegExtractAudio',\n",
        "        'preferredcodec': 'wav',\n",
        "    }],\n",
        "    \"outtmpl\": 'tan',  # this is where you can edit how you'd like the filenames to be formatted\n",
        "}\n",
        "def download_from_url(url):\n",
        "    ydl.download([url])\n",
        "    # stream = ffmpeg.input('output.m4a')\n",
        "    # stream = ffmpeg.output(stream, 'output.wav')\n",
        "\n",
        "\n",
        "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "      url = \"https://www.youtube.com/watch?v=GpmOn4RyzZI\" #@param {type:\"string\"}\n",
        "      download_from_url(url)"
      ],
      "metadata": {
        "id": "5uURQqViVIzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cover this song by your own voice"
      ],
      "metadata": {
        "id": "PBIiwlseWAIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import subprocess\n",
        "from pydub import AudioSegment\n",
        "\n",
        "VOICE = \"tan\"\n",
        "# Define audio input\n",
        "AUDIO_INPUT = \"tan.wav\"  # Path to the input audio file\n",
        "\n",
        "# Separate vocals and instruments using Demucs\n",
        "def separate_audio(input_audio):\n",
        "    command = f\"demucs --two-stems=vocals {input_audio}\"\n",
        "    result = subprocess.run(command.split(), stdout=subprocess.PIPE)\n",
        "    print(result.stdout.decode())\n",
        "\n",
        "# Run the separation function\n",
        "separate_audio(AUDIO_INPUT)\n",
        "\n",
        "# Inference\n",
        "AUDIO = f\"separated/htdemucs/{VOICE}/vocals\"  # Path to the separated vocal file\n",
        "MODEL = \"logs/44k/G_199.pth\"  # Path to the model file\n",
        "CONFIG = \"logs/44k/config.json\"  # Path to the configuration file\n",
        "\n",
        "# Change according to your voice tone\n",
        "PITCH = 0  # Pitch adjustment (12 = 1 octave, -12 = -1 octave)\n",
        "\n",
        "# Run inference\n",
        "def run_inference(audio, config, model, pitch):\n",
        "    print(\"Running inference...\")\n",
        "    command = f\"svc infer {audio}.wav -c {config} -m {model} -na -t {pitch}\"\n",
        "    subprocess.run(command.split())\n",
        "    print(\"Completed inference...\")\n",
        "\n",
        "run_inference(AUDIO, CONFIG, MODEL, PITCH)\n",
        "\n",
        "# Combine vocal and instrument (song cover)\n",
        "VOCAL = f\"separated/htdemucs/{VOICE}/vocals.out.wav\"  # Path to the vocal output\n",
        "INSTRUMENT = f\"separated/htdemucs/{VOICE}/no_vocals.wav\"  # Path to the instrumental output\n",
        "\n",
        "def combine_audio(vocal_file, instrument_file, output_file):\n",
        "    sound1 = AudioSegment.from_file(vocal_file)\n",
        "    sound2 = AudioSegment.from_file(instrument_file)\n",
        "    sound1 = sound1 + (sound1.dBFS * 0.5)  # Increase by 50% of the current dBFS level\n",
        "\n",
        "    combined = sound1.overlay(sound2)\n",
        "    combined.export(output_file, format='wav')\n",
        "    print(f\"Combined audio saved to {output_file}\")\n",
        "\n",
        "# Run the combining function\n",
        "combine_audio(VOCAL, INSTRUMENT, f\"{VOICE}_covered.wav\")\n",
        "\n",
        "# Optional: play the final cover audio (if using an environment that supports audio pl"
      ],
      "metadata": {
        "id": "mI3Pw1C1Qr4j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}